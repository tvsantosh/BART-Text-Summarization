# -*- coding: utf-8 -*-
"""textsummery.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yj-ywQPRUDjMqSe1ScnupILL0H-HVhPi
"""

!pip install datasets

from datasets import load_dataset
df = load_dataset("knkarthick/dialogsum")

df

df['train']

df['validation']

df['test'].features

df['train'][1]['dialogue']

df['train'][1]['summary']

"""#without Fine-tuning"""

!pip install transformers

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("summarization", model="facebook/bart-large-cnn")

article_1=df['test'][1]['dialogue']

pipe(article_1)

pipe(article_1,max_length=20,min_length=10,do_sample=False)

df['train'][1]['summary']

"""Fine tuning
**bold text**
"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("facebook/bart-large-cnn")
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/bart-large-cnn")

def tokenization(batch):
    source = batch['dialogue']
    target = batch['summary']

    # Tokenizing the source and target text
    source_ids = tokenizer(source, padding='max_length', truncation=True, max_length=128)
    target_ids = tokenizer(target, padding='max_length', truncation=True, max_length=128)

    # Adjusting labels
    labels = target_ids['input_ids']
    labels = [
        [(label if label != tokenizer.pad_token_type_id else -100) for label in labels_example]
        for labels_example in labels
    ]

    return {
        'input_ids': source_ids['input_ids'],
        'attention_mask': source_ids['attention_mask'],
        'labels': labels,
    }

df_source = df['train'].map(tokenization, batched=True)

"""

```
# Training argument

```

"""

from transformers import TrainingArguments,Trainer

training_args=TrainingArguments(
    output_dir='/content',
    per_device_train_batch_size=8,
    num_train_epochs=2,
    remove_unused_columns=True
)

# Split the Hugging Face Dataset into train and test sets
dataset = df_source.train_test_split(test_size=0.2, seed=42)

# Access the train and test datasets
train_dataset = dataset['train']
test_dataset = dataset['test']

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset
)

trainer.train()

eval_results=trainer.evaluate()

eval_results

model.save_pretrained('bart_model')

tokenizer.save_pretrained('bart_tokenizer')

tokenizer = AutoTokenizer.from_pretrained('/content/bart_tokenizer')
model = AutoModelForSeq2SeqLM.from_pretrained('/content/bart_model')

def summerize(data):
  input=tokenizer(data,return_tensors='pt',max_length=124,truncation=True)
  summerize=model.generate(input_ids=input['input_ids'],max_length=150,min_length=30,do_sample=False)
  a=tokenizer.decode(summerize[0],skip_special_tokens=True)
  print(a)

summerize(data1)

data1="""I am pleased to present this report on the evolving
global aid architecture and the role of the International
Development Association (IDA) in addressing its
challenges. The report is the first in a series covering
broad financing trends, the increasing circumvention of
recipient governments, and the impact of these changes
on fragile and conflict-affected states—and how IDA is
addressing these trends to improve the effectiveness of
aid for sustainable development.
This report describes the increasingly complex
relationship between donors and recipient countries
and the inefficiencies that have emerged—including
a reduction in the volume of concessional resources
available to the poorest countries, especially for those
struggling with debt and fragile situations. It also lays out
a strong rationale for investing in the IDA21 replenishment
for better financial and development outcomes."""

!pip install --upgrade transformers